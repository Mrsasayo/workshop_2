{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1: Importar librerías\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import time # Para medir el tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2: Configuración Inicial\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "pd.set_option('display.max_rows', 15)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 12:28:59,743 - INFO - Cargando variables de entorno para la conexión a la base de datos...\n"
     ]
    }
   ],
   "source": [
    "# Celda 3: Cargar Variables de Entorno y Definir Constantes para SPOTIFY\n",
    "logging.info(\"Cargando variables de entorno para la conexión a la base de datos...\")\n",
    "dotenv_path = '/home/nicolas/Escritorio/workshops/workshop_2/env/.env' # <-- CONFIRMA ESTA RUTA\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "DB_USER = os.getenv('POSTGRES_USER')\n",
    "DB_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "DB_HOST = os.getenv('POSTGRES_HOST')\n",
    "DB_PORT = os.getenv('POSTGRES_PORT')\n",
    "DB_NAME = os.getenv('POSTGRES_DB')\n",
    "DEFAULT_DB = 'postgres'\n",
    "\n",
    "CSV_FILE_PATH = '/home/nicolas/Escritorio/workshops/workshop_2/data/spotify_dataset.csv' # <-- NUEVO ARCHIVO CSV\n",
    "TABLE_NAME = 'spotify_dataset'                                                  # <-- NUEVO NOMBRE DE TABLA\n",
    "CHUNK_SIZE = 40000                                                             # <-- TAMAÑO DEL CHUNK\n",
    "\n",
    "if not all([DB_USER, DB_PASSWORD, DB_HOST, DB_PORT, DB_NAME]):\n",
    "    logging.error(\"Una o más variables de entorno no están definidas correctamente en \" + dotenv_path)\n",
    "    raise ValueError(\"Variables de entorno incompletas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 12:28:59,754 - INFO - Intentando conectar a la base de datos por defecto 'postgres' para verificar/crear 'artists'...\n",
      "WARNING:  la base de datos «postgres» tiene una discordancia de versión de ordenamiento (“collation”)\n",
      "DETAIL:  La base de datos fue creada usando la versión de ordenamiento 2.31, pero el sistema operativo provee la versión 2.35.\n",
      "HINT:  Reconstruya todos los objetos en esta base de datos que usen el ordenamiento por omisión y ejecute ALTER DATABASE postgres REFRESH COLLATION VERSION, o construya PostgreSQL con la versión correcta de la biblioteca.\n",
      "2025-04-11 12:28:59,775 - INFO - La base de datos 'artists' ya existe.\n",
      "2025-04-11 12:28:59,776 - INFO - Cursor de la conexión por defecto cerrado.\n",
      "2025-04-11 12:28:59,777 - INFO - Conexión a la base de datos por defecto cerrada.\n"
     ]
    }
   ],
   "source": [
    "# Celda 4: Verificar/Crear la base de datos 'artists' (Sin cambios)\n",
    "# ... (código de la celda 4 anterior) ...\n",
    "conn_default = None\n",
    "cursor_default = None\n",
    "try:\n",
    "    logging.info(f\"Intentando conectar a la base de datos por defecto '{DEFAULT_DB}' para verificar/crear '{DB_NAME}'...\")\n",
    "    conn_default = psycopg2.connect(\n",
    "        dbname=DEFAULT_DB,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD,\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT\n",
    "    )\n",
    "    conn_default.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    cursor_default = conn_default.cursor()\n",
    "\n",
    "    cursor_default.execute(sql.SQL(\"SELECT 1 FROM pg_database WHERE datname = %s\"), (DB_NAME,))\n",
    "    exists = cursor_default.fetchone()\n",
    "\n",
    "    if not exists:\n",
    "        logging.info(f\"La base de datos '{DB_NAME}' no existe. Creándola...\")\n",
    "        cursor_default.execute(sql.SQL(f\"CREATE DATABASE {DB_NAME}\"))\n",
    "        logging.info(f\"Base de datos '{DB_NAME}' creada exitosamente.\")\n",
    "    else:\n",
    "        logging.info(f\"La base de datos '{DB_NAME}' ya existe.\")\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    logging.error(f\"Error al conectar o verificar/crear la base de datos '{DB_NAME}': {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logging.error(f\"Ocurrió un error inesperado durante la verificación/creación de la base de datos: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    if cursor_default:\n",
    "        cursor_default.close()\n",
    "        logging.info(\"Cursor de la conexión por defecto cerrado.\")\n",
    "    if conn_default:\n",
    "        conn_default.close()\n",
    "        logging.info(\"Conexión a la base de datos por defecto cerrada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 12:28:59,785 - INFO - Iniciando carga de /home/nicolas/Escritorio/workshops/workshop_2/data/spotify_dataset.csv a la tabla 'spotify_dataset' en chunks de 40000...\n",
      "2025-04-11 12:28:59,786 - INFO - Creando motor SQLAlchemy para la base de datos 'artists'...\n",
      "2025-04-11 12:28:59,902 - INFO - Procesando chunk 1...\n",
      "2025-04-11 12:28:59,905 - INFO - Procesando el primer chunk (mostrando info y reemplazando tabla)...\n",
      "2025-04-11 12:28:59,906 - INFO - Primeras 5 filas del primer chunk:\n",
      "2025-04-11 12:28:59,911 - INFO - Información del primer chunk:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| track_id               | artists                | album_name                                             | track_name                 |   popularity |   duration_ms | explicit   |   danceability |   energy |   key |   loudness |   mode |   speechiness |   acousticness |   instrumentalness |   liveness |   valence |   tempo |   time_signature | track_genre   |\n",
      "|:-----------------------|:-----------------------|:-------------------------------------------------------|:---------------------------|-------------:|--------------:|:-----------|---------------:|---------:|------:|-----------:|-------:|--------------:|---------------:|-------------------:|-----------:|----------:|--------:|-----------------:|:--------------|\n",
      "| 5SuOikwiRyPMVoIQDJUgSV | Gen Hoshino            | Comedy                                                 | Comedy                     |           73 |        230666 | False      |          0.676 |   0.461  |     1 |     -6.746 |      0 |        0.143  |         0.0322 |           1.01e-06 |     0.358  |     0.715 |  87.917 |                4 | acoustic      |\n",
      "| 4qPNDBW1i3p13qLCt0Ki3A | Ben Woodward           | Ghost (Acoustic)                                       | Ghost - Acoustic           |           55 |        149610 | False      |          0.42  |   0.166  |     1 |    -17.235 |      1 |        0.0763 |         0.924  |           5.56e-06 |     0.101  |     0.267 |  77.489 |                4 | acoustic      |\n",
      "| 1iJBSr7s7jYXzM8EGcbK5b | Ingrid Michaelson;ZAYN | To Begin Again                                         | To Begin Again             |           57 |        210826 | False      |          0.438 |   0.359  |     0 |     -9.734 |      1 |        0.0557 |         0.21   |           0        |     0.117  |     0.12  |  76.332 |                4 | acoustic      |\n",
      "| 6lfxq3CG4xtTiEg7opyCyx | Kina Grannis           | Crazy Rich Asians (Original Motion Picture Soundtrack) | Can't Help Falling In Love |           71 |        201933 | False      |          0.266 |   0.0596 |     0 |    -18.515 |      1 |        0.0363 |         0.905  |           7.07e-05 |     0.132  |     0.143 | 181.74  |                3 | acoustic      |\n",
      "| 5vjLSffimiIP26QG5WcN2K | Chord Overstreet       | Hold On                                                | Hold On                    |           82 |        198853 | False      |          0.618 |   0.443  |     2 |     -9.681 |      1 |        0.0526 |         0.469  |           0        |     0.0829 |     0.167 | 119.949 |                4 | acoustic      |\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 20 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   track_id          40000 non-null  object \n",
      " 1   artists           40000 non-null  object \n",
      " 2   album_name        40000 non-null  object \n",
      " 3   track_name        40000 non-null  object \n",
      " 4   popularity        40000 non-null  int64  \n",
      " 5   duration_ms       40000 non-null  int64  \n",
      " 6   explicit          40000 non-null  bool   \n",
      " 7   danceability      40000 non-null  float64\n",
      " 8   energy            40000 non-null  float64\n",
      " 9   key               40000 non-null  int64  \n",
      " 10  loudness          40000 non-null  float64\n",
      " 11  mode              40000 non-null  int64  \n",
      " 12  speechiness       40000 non-null  float64\n",
      " 13  acousticness      40000 non-null  float64\n",
      " 14  instrumentalness  40000 non-null  float64\n",
      " 15  liveness          40000 non-null  float64\n",
      " 16  valence           40000 non-null  float64\n",
      " 17  tempo             40000 non-null  float64\n",
      " 18  time_signature    40000 non-null  int64  \n",
      " 19  track_genre       40000 non-null  object \n",
      "dtypes: bool(1), float64(9), int64(5), object(5)\n",
      "memory usage: 5.8+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 12:29:10,627 - INFO - Primer chunk cargado en la tabla 'spotify_dataset' (tabla reemplazada).\n",
      "2025-04-11 12:29:10,628 - INFO - Chunk 1 procesado (40000 filas) en 10.73 segundos. Total procesado: 40000\n",
      "2025-04-11 12:29:10,733 - INFO - Procesando chunk 2...\n",
      "2025-04-11 12:29:22,531 - INFO - Chunk 2 procesado (40000 filas) en 11.80 segundos. Total procesado: 80000\n",
      "2025-04-11 12:29:22,630 - INFO - Procesando chunk 3...\n",
      "2025-04-11 12:29:33,792 - INFO - Chunk 3 procesado (34000 filas) en 11.16 segundos. Total procesado: 114000\n",
      "2025-04-11 12:29:33,795 - INFO - Todos los chunks procesados. Tiempo total: 34.01 segundos.\n",
      "2025-04-11 12:29:33,796 - INFO - DataFrame completo cargado exitosamente en la tabla 'spotify_dataset'. Total de filas: 114000\n"
     ]
    }
   ],
   "source": [
    "# Celda 5: Cargar datos desde el archivo CSV de Spotify en Chunks\n",
    "# Celda 6: Conectar a la base de datos 'artists' y cargar el DataFrame de Spotify con Chunksize\n",
    "\n",
    "# Celda 5 y 6 Combinadas: Leer CSV en Chunks y Cargar a PostgreSQL\n",
    "logging.info(f\"Iniciando carga de {CSV_FILE_PATH} a la tabla '{TABLE_NAME}' en chunks de {CHUNK_SIZE}...\")\n",
    "\n",
    "engine = None\n",
    "total_rows_processed = 0\n",
    "start_time = time.time()\n",
    "first_chunk = True\n",
    "\n",
    "try:\n",
    "    # Crear motor SQLAlchemy ANTES del bucle\n",
    "    logging.info(f\"Creando motor SQLAlchemy para la base de datos '{DB_NAME}'...\")\n",
    "    db_url = f'postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n",
    "    engine = create_engine(db_url)\n",
    "\n",
    "    # Crear el iterador de chunks\n",
    "    reader = pd.read_csv(CSV_FILE_PATH, chunksize=CHUNK_SIZE)\n",
    "\n",
    "    # Iterar sobre los chunks\n",
    "    for i, chunk in enumerate(reader):\n",
    "        chunk_start_time = time.time()\n",
    "        logging.info(f\"Procesando chunk {i+1}...\")\n",
    "\n",
    "        # Eliminar columna 'Unnamed: 0' si existe en el chunk\n",
    "        if 'Unnamed: 0' in chunk.columns:\n",
    "            chunk = chunk.drop(columns=['Unnamed: 0'])\n",
    "            # No loguear esto cada vez, solo procesarlo\n",
    "            # logging.info(\" Columna 'Unnamed: 0' eliminada del chunk.\")\n",
    "\n",
    "        # --- Lógica para el primer chunk ---\n",
    "        if first_chunk:\n",
    "            logging.info(\"Procesando el primer chunk (mostrando info y reemplazando tabla)...\")\n",
    "            logging.info(\"Primeras 5 filas del primer chunk:\")\n",
    "            print(chunk.head().to_markdown(index=False))\n",
    "            logging.info(\"Información del primer chunk:\")\n",
    "            chunk.info()\n",
    "            # Cargar el primer chunk, reemplazando la tabla si existe\n",
    "            chunk.to_sql(TABLE_NAME, con=engine, if_exists='replace', index=False, method='multi')\n",
    "            logging.info(f\"Primer chunk cargado en la tabla '{TABLE_NAME}' (tabla reemplazada).\")\n",
    "            first_chunk = False # Ya no es el primer chunk\n",
    "        # --- Lógica para chunks subsiguientes ---\n",
    "        else:\n",
    "            # Añadir los chunks restantes a la tabla existente\n",
    "            chunk.to_sql(TABLE_NAME, con=engine, if_exists='append', index=False, method='multi')\n",
    "            # logging.info(f\"Chunk {i+1} añadido a la tabla '{TABLE_NAME}'.\") # Opcional, puede ser muy verboso\n",
    "\n",
    "        rows_in_chunk = len(chunk)\n",
    "        total_rows_processed += rows_in_chunk\n",
    "        chunk_end_time = time.time()\n",
    "        logging.info(f\"Chunk {i+1} procesado ({rows_in_chunk} filas) en {chunk_end_time - chunk_start_time:.2f} segundos. Total procesado: {total_rows_processed}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    logging.info(f\"Todos los chunks procesados. Tiempo total: {end_time - start_time:.2f} segundos.\")\n",
    "    logging.info(f\"DataFrame completo cargado exitosamente en la tabla '{TABLE_NAME}'. Total de filas: {total_rows_processed}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"Error: El archivo CSV no se encontró en la ruta: {CSV_FILE_PATH}\")\n",
    "    engine = None # Falló antes de usar el engine\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error durante la lectura del CSV o la carga a la base de datos: {e}\")\n",
    "    # Mantener engine definido si el error fue durante el bucle para permitir la verificación\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 12:29:33,806 - INFO - Verificando la carga final en la tabla 'spotify_dataset'...\n",
      "2025-04-11 12:29:33,821 - INFO - Número de filas en la tabla final 'spotify_dataset': 114000\n",
      "2025-04-11 12:29:33,822 - INFO - ¡Verificación exitosa! El número de filas procesadas coincide con la base de datos.\n",
      "2025-04-11 12:29:33,822 - INFO - Script de carga para 'spotify_dataset' finalizado.\n"
     ]
    }
   ],
   "source": [
    "# Celda 7: Verificar la carga de datos (Usando total_rows_processed)\n",
    "if engine: # Solo verificar si el engine se pudo crear\n",
    "    try:\n",
    "        logging.info(f\"Verificando la carga final en la tabla '{TABLE_NAME}'...\")\n",
    "        with engine.connect() as connection:\n",
    "            query = text(f'SELECT COUNT(*) FROM \"{TABLE_NAME}\"')\n",
    "            result = connection.execute(query)\n",
    "            num_db_rows = result.scalar_one()\n",
    "\n",
    "        logging.info(f\"Número de filas en la tabla final '{TABLE_NAME}': {num_db_rows}\")\n",
    "\n",
    "        # Comparar con el número total de filas procesadas de los chunks\n",
    "        if total_rows_processed == num_db_rows:\n",
    "            logging.info(\"¡Verificación exitosa! El número de filas procesadas coincide con la base de datos.\")\n",
    "        # Añadir una comprobación extra por si la cuenta total fue 0 (indicando quizás un archivo vacío o error temprano)\n",
    "        elif total_rows_processed == 0 and num_db_rows == 0:\n",
    "            logging.warning(\"Se procesaron 0 filas y la tabla está vacía. Verifica el archivo CSV.\")\n",
    "        else:\n",
    "            logging.warning(f\"Discrepancia en el número de filas: Procesadas ({total_rows_processed}) vs DB ({num_db_rows}).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error durante la verificación final de la carga: {e}\")\n",
    "else:\n",
    "    logging.error(\"No se pudo realizar la verificación porque la creación del engine falló previamente.\")\n",
    "\n",
    "logging.info(f\"Script de carga para '{TABLE_NAME}' finalizado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
